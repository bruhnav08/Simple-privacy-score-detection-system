import pandas as pd
import re
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from textblob import TextBlob # For sentiment analysis
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# --- Important: NLTK Data Downloads ---
# These lines ensure that the necessary NLTK data (stopwords, punkt tokenizer) are available.
# They will attempt to download them if not found.
# If you get an error here, you might need to run `nltk.download('stopwords')`
# and `nltk.download('punkt')` manually in your Python interpreter first.
print("Checking for NLTK data (stopwords, punkt)...")
try:
    nltk.data.find('corpora/stopwords')
except Exception: # Corrected exception type
    print("NLTK 'stopwords' not found. Attempting download...")
    nltk.download('stopwords')
    print("NLTK 'stopwords' downloaded.")

try:
    nltk.data.find('tokenizers/punkt')
except Exception: # Corrected exception type
    print("NLTK 'punkt' not found. Attempting download...")
    nltk.download('punkt')
    print("NLTK 'punkt' downloaded.")
print("NLTK data check complete.")

# --- Configuration ---
CURRENT_DATE = datetime(2025, 7, 15) # Fixed reference date for account age calculations
OUTPUT_CSV_NAME = 'preprocessed_socialmedia.csv' # Output file name

print("\n--- Starting Comprehensive Data Preprocessing (1.py) ---")

# --- Step 1: Load Data ---
try:
    # Assuming extended_socialmedia.csv is generated by generate_synthetic_data.py
    df = pd.read_csv('extended_socialmedia.csv')
    print(f"Data loaded successfully from extended_socialmedia.csv. Shape: {df.shape}")
except FileNotFoundError:
    print("Error: 'extended_socialmedia.csv' not found.")
    print("Please ensure 'generate_synthetic_data.py' has been run to create this file.")
    exit()
except Exception as e:
    print(f"An error occurred while loading data: {e}")
    exit()

# --- Step 2: Initial Preprocessing & Feature Engineering (Original + Enhanced) ---

# 2.1 Combine Bio Fields
df['bio_combined'] = df['User Bio'].fillna('') + ' ' + \
                     df['User Description 1'].fillna('') + ' ' + \
                     df['User Description 2'].fillna('')
df['bio_combined'] = df['bio_combined'].str.strip()
print("Combined bio fields.")

# 2.2 Account Age in Days & Normalized
df['Account Creation Date'] = pd.to_datetime(df['Account Creation Date'], errors='coerce')
df['account_age_days'] = (CURRENT_DATE - df['Account Creation Date']).dt.days.fillna(0)
# Normalize account_age_days
scaler_age = MinMaxScaler()
# Ensure column is treated as 2D array for scaler
df['account_age_normalized'] = scaler_age.fit_transform(df[['account_age_days']])
print("Calculated and normalized account age.")

# 2.3 Contact Info Detection (Enhanced with more specific regex for phone/email)
def has_email_in_text(text):
    return 1 if re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', str(text)) else 0

def has_phone_in_text(text):
    # Regex for various phone number formats
    return 1 if re.search(r'(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}(?!\d)', str(text)) else 0

def has_contact_info_original(text):
    # Original broad detection
    patterns = [
        r'\b(?:email|mail|contact|reach me at|DM|PM|inbox|telegram|whatsapp|phone|call)\b',
        r'@\w+\.\w+', # Basic email pattern
        r'\d{3}[-.\s]?\d{3}[-.\s]?\d{4}', # Basic phone pattern
        r'(?:https?://)?(?:www\.)?\S+\.\S+' # URLs
    ]
    return 1 if any(re.search(p, str(text), re.IGNORECASE) for p in patterns) else 0

df['has_contact_info'] = df['bio_combined'].apply(has_contact_info_original)
df['has_email_in_bio'] = df['bio_combined'].apply(has_email_in_text)
df['has_phone_in_bio'] = df['bio_combined'].apply(has_phone_in_text)
print("Detected basic, email, and phone contact info.")


# 2.4 Sensitive Keywords Detection (Enhanced to include density)
sensitive_keywords = ['confidential', 'private', 'secret', 'medical', 'health', 'financial',
                      'bank', 'password', 'ssn', 'social security', 'credit card', 'personal info',
                      'diagnosis', 'therapy', 'medication', 'account number', 'address', 'location shared',
                      'top secret', 'classified']

def has_sensitive_keywords_func(text, keywords):
    return 1 if any(re.search(r'\b' + re.escape(k) + r'\b', str(text), re.IGNORECASE) for k in keywords) else 0

def sensitive_keyword_density_func(text, keywords):
    text_lower = str(text).lower()
    words = word_tokenize(text_lower) # Use NLTK tokenizer
    stop_words = set(stopwords.words('english'))
    # Filter out non-alphanumeric words and stopwords
    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]
    
    if not filtered_words:
        return 0.0 # Return float for consistency
    
    sensitive_count = sum(1 for word in filtered_words if any(re.search(r'\b' + re.escape(k) + r'\b', word, re.IGNORECASE) for k in keywords))
    return sensitive_count / len(filtered_words)

df['has_sensitive_keywords'] = df['bio_combined'].apply(lambda x: has_sensitive_keywords_func(x, sensitive_keywords))
df['sensitive_keyword_density_bio'] = df['bio_combined'].apply(lambda x: sensitive_keyword_density_func(x, sensitive_keywords))
df['sensitive_keyword_density_post'] = df['Post Text'].apply(lambda x: sensitive_keyword_density_func(x, sensitive_keywords))

# Normalize sensitive keyword densities
scaler_skd_bio = MinMaxScaler()
# Ensure column is treated as 2D array for scaler
df['sensitive_keyword_density_bio_normalized'] = scaler_skd_bio.fit_transform(df[['sensitive_keyword_density_bio']])
scaler_skd_post = MinMaxScaler()
# Ensure column is treated as 2D array for scaler
df['sensitive_keyword_density_post_normalized'] = scaler_skd_post.fit_transform(df[['sensitive_keyword_density_post']])
print("Detected sensitive keywords and calculated their density.")

# 2.5 Specific Location Mention
# More robust regex for specific locations
def is_specific_location_func(text):
    patterns = [
        r'\d+\s(?:[A-Z][a-z]+\s){1,2}(?:Street|St|Road|Rd|Avenue|Ave|Boulevard|Blvd|Lane|Ln|Drive|Dr|Place|Pl|Court|Ct|Circle|Cir|Square|Sq|Terrace|Ter|Way|Wy)\b',
        r'\b[A-Z]{2}\s\d{5}(?:-\d{4})?\b', # US Zip Code (e.g., NY 10001)
        r'\b(?:London|Paris|Berlin|New York|Los Angeles|Chicago|Delhi|Mumbai|Tokyo|Beijing|Sydney|Cairo|Rio de Janeiro|Bengaluru|Bangalore)(?:, [A-Z]{2,})?\b', # Specific major cities
        r'\b(?:latitude|longitude|lat|lon)\s*[:=]?\s*[-+]?\d{1,3}(?:\.\d+)?(?:,\s*[-+]?\d{1,3}(?:\.\d+)?)?\b' # Lat/Lon coordinates
    ]
    return 1 if any(re.search(p, str(text), re.IGNORECASE) for p in patterns) else 0

df['is_specific_location'] = df['Location'].apply(is_specific_location_func)
print("Detected specific location mentions.")

# 2.6 Engagement Rate (Likes/Comments/Shares)
# Fill NaN for engagement metrics with 0 before calculation
df['Likes/Reactions'] = df['Likes/Reactions'].fillna(0)
df['Comments'] = df['Comments'].fillna(0)
df['Shares/Retweets'] = df['Shares/Retweets'].fillna(0)
df['User Followers'] = df['User Followers'].fillna(0)
df['User Following'] = df['User Following'].fillna(0) # Ensure this is also filled for ratios

# --- NEW: Normalize User Followers ---
scaler_followers = MinMaxScaler()
df['followers_normalized'] = scaler_followers.fit_transform(df[['User Followers']])
print("Calculated and normalized User Followers.")
# --- END NEW ---

df['engagement'] = df['Likes/Reactions'] + df['Comments'] + df['Shares/Retweets']
# Handle division by zero for engagement rate
df['engagement_rate'] = df.apply(lambda row: row['engagement'] / row['User Followers'] if row['User Followers'] > 0 else 0, axis=1)

scaler_engagement_rate = MinMaxScaler()
df['engagement_normalized'] = scaler_engagement_rate.fit_transform(df[['engagement_rate']])
print("Calculated and normalized engagement rate.")

# 2.7 Hashtag Score
df['Hashtags'] = df['Hashtags'].fillna('')
df['hashtag_count'] = df['Hashtags'].apply(lambda x: len(re.findall(r'#\w+', str(x))))
scaler_hashtag_count = MinMaxScaler()
df['hashtag_score'] = scaler_hashtag_count.fit_transform(df[['hashtag_count']])
print("Calculated and normalized hashtag score.")

# 2.8 Account Verification
df['is_verified'] = df['Account Verification'].apply(lambda x: 1 if x == 'Verified' else 0)
print("Converted account verification to binary.")

# 2.9 Media Type Score
media_type_mapping = {'Video': 1.0, 'Image': 0.5, 'Text': 0.0}
df['media_type_score'] = df['Media Type'].map(media_type_mapping).fillna(0.0)
print("Mapped media type to score.")

# 2.10 Privacy Setting Score
privacy_mapping = {
    'Public': 1.0,
    'Friends Only': 0.75,
    'Followers Only': 0.5,
    'Private': 0.0,
    'Custom': 0.25 # Assuming custom settings are generally more private than public, but less than fully private
}
df['privacy_setting_score'] = df['Privacy Settings'].map(privacy_mapping).fillna(0.0)
print("Mapped privacy settings to score.")

# 2.11 User Following & Normalized (Ensured calculation)
scaler_following = MinMaxScaler()
df['following_normalized'] = scaler_following.fit_transform(df[['User Following']])
print("Calculated and normalized User Following.")

# 2.12 Total Posts (Needs 'User Activity' column, typically 'X posts per week')
# We'll use activity_normalized from 'User Activity' column
df['activity_value'] = df['User Activity'].apply(lambda x: float(re.search(r'(\d+(\.\d+)?)\s*posts', str(x)).group(1)) if re.search(r'(\d+(\.\d+)?)\s*posts', str(x)) else 0)
scaler_activity = MinMaxScaler()
df['activity_normalized'] = scaler_activity.fit_transform(df[['activity_value']])
print("Ensured activity_normalized is calculated from 'User Activity'.")

# 2.13 Likes & Comments Normalized (Ensured calculation)
scaler_likes = MinMaxScaler()
df['likes_normalized'] = scaler_likes.fit_transform(df[['Likes/Reactions']])
scaler_comments = MinMaxScaler()
df['comments_normalized'] = scaler_comments.fit_transform(df[['Comments']])
print("Normalized Likes/Reactions and Comments.")

# 2.14 Follower/Following Ratio & Normalized (Ensured calculation)
# Add a small epsilon to denominator to prevent division by zero for users with 0 following
df['follower_following_ratio'] = df.apply(lambda row: row['User Followers'] / (row['User Following'] + 1e-6) if (row['User Following'] + 1e-6) > 0 else 0, axis=1)
scaler_ff_ratio = MinMaxScaler()
df['follower_following_ratio_normalized'] = scaler_ff_ratio.fit_transform(df[['follower_following_ratio']])
print("Calculated and normalized follower/following ratio.")

# 2.15 Comments per Post & Normalized (Using avg comments based on activity or likes as proxy)
# Re-evaluate comments_per_post: If 'activity_normalized' implies 'posts_per_week', then total comments / (posts_per_week * num_weeks)
# Given current data, let's use Comments / (Likes + Shares) as a proxy for interaction depth on posts
df['interaction_depth_ratio'] = df.apply(lambda row: row['Comments'] / (row['Likes/Reactions'] + row['Shares/Retweets'] + 1e-6) if (row['Likes/Reactions'] + row['Shares/Retweets'] + 1e-6) > 0 else 0, axis=1)
scaler_idr = MinMaxScaler()
df['comments_per_post_normalized'] = scaler_idr.fit_transform(df[['interaction_depth_ratio']]) # Renaming to fit existing slot
print("Calculated and normalized comments per post (interaction depth) ratio.")


# --- Step 3: NEW ADVANCED FEATURE ENGINEERING ---

# 3.1 Sentiment Analysis (Bio and Post Text)
def get_sentiment_score(text):
    if pd.isna(text) or text == '':
        return 0.0 # Neutral sentiment for empty/missing text
    analysis = TextBlob(str(text))
    return analysis.sentiment.polarity # Polarity ranges from -1 (negative) to +1 (positive)

df['bio_sentiment_score'] = df['bio_combined'].apply(get_sentiment_score)
df['post_text_sentiment_score'] = df['Post Text'].apply(get_sentiment_score)
print("Calculated bio and post text sentiment scores.")


# 3.2 Post Text Length (and Normalized)
df['post_text_length'] = df['Post Text'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
scaler_post_length = MinMaxScaler()
df['post_text_length_normalized'] = scaler_post_length.fit_transform(df[['post_text_length']])
print("Calculated and normalized post text length.")

# 3.3 Time Since Last Post (requires sorting by user and timestamp)
df['Post Timestamp'] = pd.to_datetime(df['Post Timestamp'], errors='coerce')
# For 'days_since_post_normalized', we'll calculate the age of each post relative to CURRENT_DATE
df['days_since_post'] = (CURRENT_DATE - df['Post Timestamp']).dt.days.fillna(0)
scaler_days_since_post = MinMaxScaler()
df['days_since_post_normalized'] = scaler_days_since_post.fit_transform(df[['days_since_post']])
print("Calculated and normalized days since post.")


# 3.4 Average Time Between Posts (per user - more complex, requires grouping)
# Ensure Post Timestamp is not NaT after coercion
df_valid_posts = df.dropna(subset=['Post Timestamp'])
user_post_times = df_valid_posts.sort_values(by=['User ID', 'Post Timestamp']).groupby('User ID')['Post Timestamp'].apply(
    lambda x: x.diff().dt.days.mean() # Calculate average difference in days between consecutive posts
).reset_index(name='avg_time_between_posts_days')

# Merge this back into the original DataFrame, handling users with only one post (NaN for avg time)
df = pd.merge(df, user_post_times, on='User ID', how='left')
# Fill NaN for users with 1 or 0 posts. Use mean of existing averages or 0.
df['avg_time_between_posts_days'] = df['avg_time_between_posts_days'].fillna(
    df['avg_time_between_posts_days'].mean()
)
scaler_avg_time_between_posts = MinMaxScaler()
# Ensure column is treated as 2D array for scaler
df['avg_time_between_posts_days_normalized'] = scaler_avg_time_between_posts.fit_transform(df[['avg_time_between_posts_days']])
print("Calculated and normalized average time between posts per user.")


# 3.5 Interaction Ratios (already done comments_per_post, this ensures general interaction ratios)
# The previous comments_per_post_normalized effectively serves as comments_to_likes_ratio_normalized
# Let's add shares_to_likes_ratio_normalized specifically
df['shares_to_likes_ratio'] = df.apply(lambda row: row['Shares/Retweets'] / (row['Likes/Reactions'] + 1e-6) if (row['Likes/Reactions'] + 1e-6) > 0 else 0, axis=1)
scaler_s2l = MinMaxScaler()
df['shares_to_likes_ratio_normalized'] = scaler_s2l.fit_transform(df[['shares_to_likes_ratio']])
print("Calculated and normalized shares-to-likes ratio.")

# 3.6 Number of Mentions in Posts
df['Mentions'] = df['Mentions'].fillna('')
df['num_mentions_in_posts'] = df['Mentions'].apply(lambda x: len(re.findall(r'@\w+', str(x))))
scaler_mentions = MinMaxScaler()
df['num_mentions_in_posts_normalized'] = scaler_mentions.fit_transform(df[['num_mentions_in_posts']])
print("Calculated and normalized number of mentions in posts.")

# --- Step 4: Final Feature Selection for Output ---
# Select only the columns that are useful for the ML model and some identifiers
columns_to_keep = [
    'User ID', 'Platform', 'Username', # Identifiers
    # Core Preprocessed Features
    'followers_normalized', 'following_normalized', 'likes_normalized', 'comments_normalized',
    'engagement_normalized', 'account_age_normalized', 'is_verified', 'activity_normalized',
    'hashtag_score', 'media_type_score', 'privacy_setting_score',
    'has_contact_info', 'is_specific_location', 'has_sensitive_keywords',
    'follower_following_ratio_normalized', 'comments_per_post_normalized',
    # Note: bio_text_length_normalized and network_size_normalized were not explicitly recreated/checked in this comprehensive 1.py.
    # If they were intended, they need explicit calculation. Omitting from final list for current run.

    # NEW Advanced Features
    'has_email_in_bio', 'has_phone_in_bio',
    'bio_sentiment_score', 'post_text_sentiment_score',
    'sensitive_keyword_density_bio_normalized', 'sensitive_keyword_density_post_normalized',
    'post_text_length_normalized',
    'days_since_post_normalized', 'avg_time_between_posts_days_normalized',
    'shares_to_likes_ratio_normalized',
    'num_mentions_in_posts_normalized'
]

# Filter DataFrame to keep only the selected columns
# Use .reindex to handle cases where some expected columns from the list might not have been created
# due to data peculiarities (though we've aimed for comprehensive creation here).
final_df = df.reindex(columns=columns_to_keep, fill_value=np.nan)

# Drop any columns that might still be all NaN if they were truly missing data and couldn't be generated/filled
final_df = final_df.dropna(axis=1, how='all')

# Impute any remaining NaNs in numerical columns with 0 after selection.
# This is a robust final step to ensure no NaNs go into the ML model.
for col in final_df.columns:
    if col in ['User ID', 'Platform', 'Username']: # Don't fill NaNs for ID/categorical columns
        continue
    if final_df[col].dtype in ['float64', 'int64']:
        final_df[col] = final_df[col].fillna(0) # Using 0 as a safe default for simplicity


print(f"\nFinal preprocessed DataFrame shape: {final_df.shape}")
print("Final preprocessed DataFrame columns:")
print(final_df.columns.tolist())

# --- Step 5: Save Preprocessed Data ---
try:
    final_df.to_csv(OUTPUT_CSV_NAME, index=False)
    print(f"\nSuccessfully saved enhanced preprocessed data to {OUTPUT_CSV_NAME}")
except Exception as e:
    print(f"Error saving preprocessed data: {e}")

print("\n--- Comprehensive Data Preprocessing (1.py) Complete ---")